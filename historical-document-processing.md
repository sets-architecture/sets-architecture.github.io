# Historical Document Processing: From non-professional photographs taken in the archive to digital text

## Complete Workflow Overview

Our research combines multiple image processing techniques with LLMs to transform historical handwritten documents into searchable digital text. The complete pipeline consists of four main stages:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Scanned       â”‚â”€â”€â”€â–¶â”‚     Image       â”‚â”€â”€â”€â–¶â”‚  Enhanced       â”‚â”€â”€â”€â–¶â”‚    LLM OCR      â”‚
â”‚   Historical    â”‚    â”‚  Processing     â”‚    â”‚   Output        â”‚    â”‚   Pipeline      â”‚
â”‚   Document      â”‚    â”‚                 â”‚    â”‚   Images        â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“                        â†“                        â†“
                       Multiple Methods:           Ready for AI:            ğŸ“ Digital Text:
                       â€¢ Adaptive                 â€¢ Clean, enhanced        â€¢ Automatic transcription
                       â€¢ Graph Analysis           â€¢ High contrast          â€¢ Structured output
                       â€¢ Color Filtering          â€¢ Noise-free             â€¢ Searchable content
                       â€¢ Edge Detection           â€¢ Optimized format       
```

## The Challenges

Archives of handwritten documents are vast, measuring in the millions of shelving kilometers worldwide. And yet, many have not been professionally scanned and digitized. Instead, scholars will often take photographs of the documents while visiting the archive. The quality of the resulting images can be poor:
- **Physically deteriorating** images can be difficult to capture because of faded ink, discolored paper
- **Inconsistent lighting per image** can result in shadows, making parts of the page difficult to read
- **Unstable lighting conditions** make it difficult to find a single comprehensive technique that works for all images in a groupf 

As a result, computational research with these materials is limited because:
- **Time-consuming to transcribe** manually by historians and archivists
- **Existing computer vision techniques** are limited for pre-processing to produce machine-readable images

**Our goal:** Automated enhancement and transcription of non-professionally photographed handwritten archival documents. 

---

## Image Processing Methods
We experimented with and evaluated multiple computer vision image preprocessing techniques to reduce noise, and improve machine readability.

### Processing Methods Overview

```
Input Document â”€â”€â”¬â”€â”€ Edge Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Enhanced Output
                 â”œâ”€â”€ Color Filtering â”€â”€â”€â”€â”€â”€â”€â”€ Enhanced Output  
                 â”œâ”€â”€ Adaptive Thresholding (3 types) â”€â”€ Enhanced Output
                 â”œâ”€â”€ Graph Analysis â”€â”€â”€â”€ Enhanced Output
                 â””â”€â”€ CLAHE Enhancement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Enhanced Output
```

### Method Details and Applications

#### Edge Detection Approach
**Technique**: Canny Edge Detection + Morphological Closing
- **Process**: Detects text edges â†’ Connects broken character segments â†’ Removes noise
- **Parameters**: Threshold1=50, Threshold2=150 for edge detection
- **Use case**: Modern handwriting with good contrast

#### Color-Based Ink Isolation  
**Technique**: HSV Color Space Filtering
- **Process**: Converts to HSV â†’ Isolates brown ink range â†’ Filters background
- **Parameters**: HSV range [20,20,100] to [30,100,255] targets brown ink
- **Use case**: Historical documents with brown or sepia ink on aged paper

#### CLAHE Enhancement
**Technique**: Contrast Limited Adaptive Histogram Equalization
- **Process**: Local contrast enhancement â†’ Adaptive thresholding â†’ Noise removal
- **Features**: Prevents over-amplification while enhancing faint text
- **Use case**: Severely deteriorated documents with minimal visible text

#### Adaptive Thresholding (Primary Method)
**Our most reliable and versatile approach** with three variants:

**Standard**
- Adaptive Gaussian thresholding with moderate parameters
- Morphological closing (4x4 kernel) to connect text segments  
- Morphological opening (2x2 kernel) to remove noise
- **Use case**: The wide range of images we processed

**Stronger**  
- More aggressive thresholding for severely faded documents
- Enhanced parameters: block size 21, C parameter 12
- **Use case**: The documents with very light or faded text

**Mean**
- Uses mean-based adaptive thresholding instead of Gaussian
- Better performance on documents with uneven illumination
- **Use case**: The documents with varying lighting conditions

#### Graph Analysis (Advanced Structural Method)
**Our most effective though costly approach**
**Technique**: Connected Component Analysis + Graph Theory
- **Process**: 
  1. Converts document pixels to graph nodes based on connectivity
  2. Identifies connected components representing text elements
  3. Analyzes component shapes using shortest path algorithms
  4. Groups similar components using network analysis
  5. Tests multiple thresholds (130-210) for robustness
- **Output**: Detailed structural analysis with text component relationships
- **Use case**: Word detection even as conditions of image quality vary across the page


### Processing Methods Comparison

| Method                   | Core Technique                                   | Best For                                   | Complexity |
| ------------------------ | ------------------------------------------------ | ------------------------------------------ | ---------- |
| **Edge Detection**       | Canny Edge Detection                             | Edge-heavy documents                       | Low        |
| **Color Filtering**      | HSV Color Space                                  | Brown/sepia ink                            | Low        |
| **Adaptive Threshold**   | Gaussian Thresholding                            | General enhancement                        | Medium     |
| **Contrast Enhancement** | Histogram Equalization                           | Severely faded text                        | Medium     |
| **CLAHE Enhancement**    | Contrast Limited Adaptive Histogram Equalization | Uneven lighting, localized contrast issues | Medium     |
| **Graph Analysis**       | Connected Components                             | Structural understanding                   | High       |

### Performance Notes
- **Adaptive Threshold** shows most consistent results across document types
- **Graph Analysis** provides detailed structural information but requires higher computational resources

## LLM OCR Experimental Framework

After image enhancement, we employ an AI-powered transcription pipeline. The LLM-powered OCR pipeline is a step-by-step process that turns messy scans, photos, or PDFs into clear, usable text. It begins by cleaning up the image to fix issues like skew, noise, or uneven lighting. Next, it identifies different parts of the pageâ€”such as paragraphs or tablesâ€”and then uses OCR or large language models to read the text. The output is polished to correct mistakes, improve readability, and extract important details like names, dates, or numbers. Finally, the results can be saved as plain text, structured data, or even a searchable PDF, with an option for human review when extra accuracy is needed.

### Complete LLM Pipeline Architecture

```
   Enhanced Images (from above) â”€â”€â”¬â”€â”€ Preprocessing Pipeline â”€â”€â”€â”€â”¬â”€â”€ Model Processing
                                  â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              MODULAR OCR PIPELINE                                   â”‚
â”‚                                                                                     â”‚
â”‚     PREPROCESSING              AI PROCESSING                POSTPROCESSING          â”‚ 
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ â€¢ Contrast      â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  API Models:        â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ â€¢ Text Cleaning     â”‚    â”‚
â”‚  â”‚   Enhancement   â”‚       â”‚  â€¢ GPT-4o/4o-mini   â”‚       â”‚ â€¢ Entity Extraction â”‚    â”‚
â”‚  â”‚ â€¢ Image Resize  â”‚       â”‚  â€¢ Claude 3 Sonnet  â”‚       â”‚ â€¢ Format Validation â”‚    â”‚
â”‚  â”‚ â€¢ Sharpening    â”‚       â”‚  â€¢ Gemini 1.5/2.5   â”‚       â”‚ â€¢ Quality Scoring   â”‚    â”‚
â”‚  â”‚ â€¢ Noise Removal â”‚       â”‚                     â”‚       â”‚                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  Local Models:      â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â”‚  â€¢ TrOCR Handwrittenâ”‚                                  â”‚
â”‚                            â”‚  â€¢ LLaVA-1.5        â”‚                                  â”‚
â”‚                            â”‚  â€¢ Moondream2       â”‚                                  â”‚
â”‚                            â”‚  â€¢ InstructBLIP     â”‚                                  â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â†“
                            BATCH PROCESSING & ANALYSIS
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ â€¢ Multi-image processing         â”‚
                        â”‚ â€¢ Model comparison analysis      â”‚
                        â”‚ â€¢ Parameter sensitivity testing  â”‚
                        â”‚ â€¢ Ground truth evaluation        â”‚
                        â”‚ â€¢ Performance benchmarking       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Options and Capabilities

#### Preprocessing Stage
**Purpose**: Optimize images specifically for LLM vision models
- **Contrast Enhancement**: Improves text visibility for AI models
- **Image Resizing**: Optimizes file size while preserving text quality  
- **Sharpening**: Enhances character definition for better recognition
- **Noise Removal**: Eliminates artifacts that confuse AI models

#### Multi-Model AI Processing
**Three categories of models tested:**

**API-Based Models** (Cloud)
- **GPT-4o/4o-mini**: Excellent general performance, structured output
- **Claude 3 Sonnet**: Superior reasoning, historical context understanding
- **Gemini 1.5/2.5 Pro**: Strong multimodal capabilities, cost-effective

**Local HuggingFace Models** (Self-hosted)
- **TrOCR (Handwritten)**: Specialized for handwritten text recognition
- **LLaVA-1.5**: Strong vision-language understanding
- **Moondream2**: Lightweight, efficient vision model
- **InstructBLIP**: Instruction-following vision model

**Traditional OCR** (Baseline comparison)
- **EasyOCR**: Modern neural OCR with multilingual support
- **Tesseract**: Classical OCR for comparison benchmarks

#### Postprocessing and Analysis
- **Text Cleaning**: Removes artifacts and standardizes formatting
- **Named Entity Recognition**: Extracts people, places, organizations, dates
- **Quality Assessment**: Confidence scoring and error detection
- **Format Conversion**: Structured output (JSON, XML, plain text)

### Systematic Evaluation Approach
```
Historical Document â”€â”¬â”€ Adaptive Threshold Processing â”€â”¬â”€ GPT-4o (temp=0.1) â”€â”¬â”€ Compare vs
                     â”‚                                 â”œâ”€ GPT-4o (temp=0.7) â”€â”¤  Ground Truth
                     â”‚                                 â””â”€ Claude 3 Sonnet â”€â”€â”€â”¤
                     â”‚                                             
                     â””â”€ Graph Analysis â”€â”€â”€â”€â”€â”¬â”€ GPT-4o (temp=0.1) â”€â”¤
                                            â”œâ”€ GPT-4o (temp=0.7) â”€â”¤
                                            â””â”€ Claude 3 Sonnet â”€â”€â”€â”˜
```

### Experimental Dataset Structure
```
Data/
â”œâ”€â”€ document_1/
â”‚   â”œâ”€â”€ original.jpg                      # Source document
â”‚   â”œâ”€â”€ adaptive_threshold.jpg            # Standard processing
â”‚   â”œâ”€â”€ graph_analysis.jpg                # Advanced processing
â”‚   â”œâ”€â”€ {model}_{method}_{temp}.txt       # LLM transcriptions (multiple per document)
â”‚   â””â”€â”€ ground_truth.txt                  # Reference transcription
â””â”€â”€ [more document folders]
```

### Implementation and Impact

These efforts address the specific needs of non-professional handwritten document photographs taken in archives by visiting scholars. These unprofessional photographs present a number of challenges that are not widely addressed in the computer vision community. They are at the intersection of two problem types: 1. handwriting recognition; 2. non-uniform illumnination. Methods that address one or the other of these issues do not provide a a comprehensive solution. Our research will contribute to ongoing work on this compound CV problem. 


<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>SETS</title>
  <meta name="description" content="SETS project page." />

  <!-- Libre Franklin (Google Fonts) -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Libre+Franklin:wght@300;400;600;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --bg: #ffffff;
      --text: #1f2937;
      --muted: #6b7280;
      --accent: #4f46e5;
      --card: #f9fafb;
      --border: #e5e7eb;
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0b0f14;
        --text: #e5e7eb;
        --muted: #9aa3af;
        --accent: #8b93ff;
        --card: #121821;
        --border: #1f2937;
      }
    }
    * { box-sizing: border-box; }
    html, body { margin: 0; padding: 0; }
    body {
      font-family: "Libre Franklin", system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", "Liberation Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
      background: var(--bg);
      color: var(--text);
      line-height: 1.6;
    }
    header {
      border-bottom: 1px solid var(--border);
      background: linear-gradient(180deg, rgba(79,70,229,0.06), transparent);
    }
    .wrap { max-width: 960px; margin: 0 auto; padding: 1.25rem; }
    .brand { display: flex; align-items: center; gap: .6rem; font-weight: 700; letter-spacing: .2px; }
    .brand .dot {
      width: 10px; height: 10px; border-radius: 50%; background: var(--accent);
      box-shadow: 0 0 0 4px color-mix(in oklab, var(--accent) 25%, transparent);
    }
    nav a { color: var(--muted); text-decoration: none; margin-left: 1rem; }
    nav a:hover { color: var(--accent); }
    main .hero { padding: 3rem 0 1.5rem; }
    h1 { font-size: clamp(1.8rem, 2vw + 1rem, 2.4rem); margin: 0 0 .5rem; }
    p.lead { color: var(--muted); max-width: 60ch; margin: 0; }
    .card {
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1rem;
      margin: 2rem 0;
      overflow: auto;
    }
    .small { font-size: .9rem; color: var(--muted); }
    .mermaid { min-height: 220px; }
    footer {
      border-top: 1px solid var(--border);
      color: var(--muted);
    }

    /* Team grid */
    .team-grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 1rem;
    }
    @media (min-width: 720px) {
      .team-grid { grid-template-columns: repeat(3, 1fr); }
    }
    .team-col {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: .75rem;
    }
    .team-col h3 {
      margin: .25rem 0 .5rem;
      font-size: 1.05rem;
      color: var(--text);
    }
    .people { list-style: none; margin: 0; padding: 0; }
    .people li {
      padding: .5rem .5rem;
      border-radius: 8px;
    }
    .people li + li { margin-top: .25rem; }
    .name { font-weight: 600; }
    .affil { color: var(--muted); }

     /* References (hanging indent, not a card) */
    .refs { margin: 1rem 0 2rem; padding-left: 1.25rem; }
    .refs li {
      margin: 0 0 .75rem;
      text-indent: -1.25rem;
      padding-left: 1.25rem;
    }
    .refs a { color: inherit; text-decoration: underline; text-decoration-style: dotted; }
    .section-spacer { margin-top: 2rem; }
    
    footer {
      border-top: 1px solid var(--border);
      color: var(--muted);
    }
    .footer-grid {
      display: flex; flex-wrap: wrap; align-items: center; justify-content: space-between; gap: .75rem;
      padding: 1rem 0;
      font-size: .95rem;
    }

  </style>

  <!-- Mermaid -->
  <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <script>
    document.addEventListener('DOMContentLoaded', function () {
      if (window.mermaid) {
        mermaid.initialize({
          startOnLoad: true,
          theme: (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) ? 'dark' : 'default',
          securityLevel: 'antiscript'
        });
      }
    });
  </script>
</head>
<body>
  <header>
    <div class="wrap" style="display:flex; align-items:center; justify-content:space-between;">
      <div class="brand"><span class="dot" aria-hidden="true"></span><span>SETS</span></div>
      <!--<nav aria-label="primary">
        <a href="#outcomes">Outcomes</a>
        <a href="#about">About</a>
      </nav>-->
    </div>
  </header>

  <main class="wrap">
    <section class="hero">
      <h1>SETS: A Set-Based Architecture for Knowledge Integration and Discovery</h1>
      <p class="lead">The goal of SETS is to improve the performance of AI in the domain of humanities research on archival sources.</p>
    </section>
   <section id="about">
    <p>Our primary focus is on what we call “deep sources”. These are handwritten archival documents that contain multiple layers of meaning: their meaning is encoded as text, but also in distinctive graphical marks, physical attributes and visual layout patterns. Meaning further accumulates over time as the documents are modified, change ownership, and are reused and reinterpreted beyond the intention of their initial creation.</p>
    <p>Deep sources are ubiquitous, found in both public and private holdings, and as important as they are difficult to
    access and read. Archival collections are vast, measuring in the millions of shelving kilometers worldwide and
    holding unique documents of high information density per square centimeter. These materials are crucial to the
    understanding of our shared past, which in turn informs choices about the future. But understanding the past is not
    simply a matter of information retrieval. It requires careful attention, mediation and interpretation of archival
    sources.</p>
    <p>The name SETS is rooted in our belief that leveraging the mathematical flexibility of set theory will be fundamental
    to achieving our goal. Set theory will allow us to represent knowledge as a dynamic graph structure where entities,
    relationships, and even concepts are defined within the graph itself as nested sets. In a SETS graph, meaning
    resides in the topological structure, which is inherently mutable and extensible, allowing the framework to evolve
    alongside scholarly inquiry and discovery. This architecture complements the complex and layered history of deep
    sources and the scholarship that makes them accessible enabling the creation of new knowledge.</p>
    <p>Preliminary work on the development of SETS began in the Spring of 2025 when the team was awarded a Propel grant from
    the Dean of Research at Stanford University. The Propel grant funded experiments to improve efficiency in learning
    from archival handwritten sources, a complex and time-consuming task that severely constrains the scope of research
    agendas. We looked at approaches to scaling the analysis of a set of disconnected images from an archive which
    required defining the components of scholarly analysis of deep sources and the dimensions to consider when designing
    evaluation metrics.</p>
      <h2>Evaluating Human-AI Interaction with Deep Sources</h2>
      <p>Our first step was creating a benchmark for human-model interaction in the analysis of deep sources (Lee et al, 2024). This evaluation framework aims to improve upon existing benchmarks for multi-modal models by taking a deep and focused approach rather than a broader but flatter approach (Yue et al, 2024). Our framework attends to the complexity of deep sources as textual, visual, and physical objects, while also taking into account the theoretical and methodological approaches of the scholar.</p>
      <p>Whereas existing benchmarks address general reasoning capabilities, we offer a benchmark for deep sources that evaluates models on an actual workflow, measuring real task completion rather than using proxy metrics. Our benchmark design is grounded in the following assumptions:</p>
      <ul>
        <li>A world model in the humanities is not singular and general, but discipline-specific, informed by previous research,  evolving as research insights emerge and as interpretive paradigms change.</li>
        <li>Deep sources hold information in their physical qualities that cannot be accessed in current digital surrogates.</li>
        <li>While the potential to scale-up research in the humanities motivates our interest in AI, deep sources will always be a minority data domain because their unique qualities are intertwined with historical moments, geographical setting, political circumstances, etc.</li>
      </ul>
      
    </section>
    <section id="outcomes">
    <h2 id="outcomes-title" style="margin-top:0;">Experiments and Outcomes</h2>
    
    <div class="card" aria-labelledby="outcomes-title">
      
      <div class="mermaid">
      flowchart TD
    A[Deep Source Analysis by Scholar] --> B[Image Pre-processing & Text Transcription]
    B --> C[LLM-aided Document Analysis]
    C --> D[Historian-guided LLM Document Analysis]
    D --> E[Query Document Data as Sets]
    
    A1[Establishing sophisticated benchmark<br/>for archival document understanding]
    B1[Comparison of methods and models<br/>for document digitization]
    C1[Measure confidence, entropy, and accuracy<br/>on 14 information retrieval tasks<br/>across three frontier models]
    D1[Evaluate scholar-language model<br/>interaction on analysis tasks]
    E1[NEXT STEP:<br/>Structured querying of<br/>document datasets]
    
    A -.-> A1
    B -.-> B1
    C -.-> C1
    D -.-> D1
    E -.-> E1
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#ffebee
      </div>
      <p class="small"></p>
    </div>
    </section>

    <section id="team" class="card" aria-labelledby="team-title">
      <h2 id="team-title">Team</h2>
      <div class="team-grid">
        <div class="team-col" aria-labelledby="leads-title">
          <h3 id="leads-title">Leads</h3>
          <ul class="people">
            <li><span class="name">Giovanna Ceserani</span><br><span class="affil">Stanford University</span></li>
            <li><span class="name">Sebastian Ahnert</span><br><span class="affil">Cambridge University</span></li>
          </ul>
        </div>
        <div class="team-col" aria-labelledby="contributors-title">
          <h3 id="contributors-title">Contributors</h3>
          <ul class="people">
            <li><span class="name">Nicole Coleman</span><br><span class="affil">Independent</span></li>
            <li><span class="name">Allen Romano</span><br><span class="affil">Independent</span></li>
            <li><span class="name">Huijun Mao</span><br><span class="affil">Stanford University</span></li>
          </ul>
        </div>
        <div class="team-col" aria-labelledby="consultants-title">
          <h3 id="consultants-title">Consultants</h3>
          <ul class="people">
            <li><span class="name">Diyi Yang</span><br><span class="affil">Stanford University</span></li>
            <li><span class="name">Michele Mauri</span><br><span class="affil">Politecnico di Milano, DensityDesign</span></li>
          </ul>
        </div>
      </div>
    </section>
    <section id="references" aria-labelledby="references-title" class="section-spacer">
      <h2 id="references-title">References</h2>
      <ol class="refs">
        <li>Lee, Mina, Megha Srivastava, Amelia Hardy, et al. 2024. “Evaluating Human-Language Model Interaction.” https://arxiv.org/abs/2212.09746.</li>
        <li>Yue, Xiang, Yuansheng Ni, Kai Zhang, et al. 2024. “MMMU: A Massive Multi-Discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June, 9556–67.</li>
      </ol>
    </section>
 
  </main>

  <footer>
    <div class="wrap footer-grid">
      <!--<span>© <span id="year"></span> SETS</span>-->
      <span>Updated August 27, 2025</span>
    </div>
  </footer>

  <script>
    document.getElementById('year').textContent = new Date().getFullYear();
  </script>
  <noscript>
    Mermaid diagrams require JavaScript. Text fallback:
    Deep source analysis → Image pre-processing & transcription → LLM-aided analysis → Historian-guided LLM analysis
  </noscript>
</body>
</html>